# ```Machine Learning Operations (MLOps)```

## ```Introducci√≥n```

Como parte de mi formaci√≥n como Data Scientist en la edtech [Henry](https://www.soyhenry.com/), se me asign√≥ un proyecto para desarrollar un sistema de recomendaci√≥n de pel√≠culas. En este proyecto, simul√© un ambiente de trabajo real en el cual una start-up de agregaci√≥n de plataformas de streaming requer√≠a un sistema de recomendaci√≥n que a√∫n no hab√≠a sido puesto en marcha. Como Data Scientist, mi responsabilidad fue desarrollar el sistema de recomendaci√≥n de manera integral, desde la recolecci√≥n y tratamiento de los datos hasta el entrenamiento y despliegue del modelo de Machine Learning. Sin embargo, los datos disponibles en ese momento eran inmaduros y requer√≠an una gran cantidad de trabajo de Data Engineer para transformarlos y prepararlos para su uso en el modelo.

## ‚úîÔ∏è```Objetivo```

Desarrollar un sistema de recomendaci√≥n de pel√≠culas y series personalizado para una start-up de agregaci√≥n de plataformas de streaming.

## üí°```Desarrollo del Proyecto```

Para lograr el objetivo, se llevaron a cabo los siguientes procesos:

1. ```Ingenier√≠a de Datos```

    1.1 Extracci√≥n, Transformaci√≥n y Limpieza de los datos (ETL) üëâ‚Ää[ETL.ipynb]()

    1.2 Disponibilizaci√≥n de los datos limpios

    üëâ‚Ää[api_consultation.csv]()
    üëâ‚Ää[movies_recommendations.csv]()
    üëâ‚Ää[movies_clean.csv]()

    1.3 Desarrollo de la API üëâ‚Ää[main.py]()

    1.4 Virtualizacion y Deployment para disponibilizar los datos para futuras consultas.

2. ```Machine Learning```

    2.1 An√°lisis Exploratorio de los datos (EDA) üëâ‚Ää[EDA.ipynb]()

    2.2 Entrenamiento del Modelo üëâ‚Ää[ML.ipynb]()

    2.3 Deployment del Modelo de Sistema de Recomendacion de Peliculas üëâ‚Ää[dannielarodriguez-project-mlops]()

## üõ†Ô∏è```Tecnolog√≠as Utilizadas```

- **Python**: lenguaje de programaci√≥n principal utilizado en el proyecto.

    ![Python](https://img.shields.io/badge/Python-3776AB.svg?style=for-the-badge&logo=Python&logoColor=white)

- **Librer√≠as de Python**: se utilizaron diversas librer√≠as de Python para diferentes tareas en el proyecto como pandas, numpy, datetime, ast, json, requests y re para la limpieza de los datos; fastAPI para las consultas de la data limpia; mientras que para el an√°lisis exploratorio de los datos matplotlib, seaborn y wordcloud; as√≠ como scikit-learn para el modelado de sistema de recomendaci√≥n de pel√≠culas.

    ![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white)
    ![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white)
    ![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=for-the-badge&logo=Matplotlib&logoColor=black)
    ![Seaborn](https://img.shields.io/badge/Seaborn-%2370399F.svg?style=for-the-badge&logo=seaborn&logoColor=white)
    ![scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=for-the-badge&logo=scikit-learn&logoColor=white)

- **Google Colab**: plataforma de Jupyter Notebook basada en la nube que se utiliz√≥ para el proceso de ETL (Extracci√≥n, Transformaci√≥n y Carga) de los datos, para el EDA (An√°lisis Exploratorio de datos) y para el Modelo de Machine Learning.

    ![Google Colab](https://img.shields.io/badge/Google%20Colab-F9AB00.svg?style=for-the-badge&logo=Google-Colab&logoColor=white)

- **Visual Studio Code**: un editor de c√≥digo fuente desarrollado por Microsoft que se utiliz√≥ para escribir y editar el c√≥digo de Python para el desarrollo de las consultas a la API.
  
    ![Visual Studio Code](https://img.shields.io/badge/Visual%20Studio%20Code-0078d7.svg?style=for-the-badge&logo=visual-studio-code&logoColor=ffffff)

- **Virtualenv**: es una herramienta √∫til para crear entornos virtuales de Python que permiten instalar y manejar dependencias de forma aislada. Para este caso, se utiliz√≥ para gestionar la creaci√≥n de la API.

    ![venv](https://img.shields.io/badge/Virtualenv-venv-%2300FFFF?style=for-the-badge&logo=python)

- **FastAPI**: un framework de Python para construir APIs web r√°pidas y escalables.

    ![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi)

- **Render**: servicio en la nube utilizado para implementar el modelo de Machine Learning.

    ![Render](https://img.shields.io/badge/Render-46E3B7.svg?style=for-the-badge&logo=Render&logoColor=white)

## ‚öôÔ∏è‚Ää```ETL```

- La informaci√≥n fue extra√≠da de archivos .csv. La documentaci√≥n y ubicaci√≥n respecto a estos archivos se encuentran en la carpeta üìÅ [dataset]()
- Fue necesario desanidar la data, ya que exist√≠an columnas como ```belongs_to_collection``` , ```production_companies```, ```spoken_languages``` y ```genres``` por mencionar algunos con registros en formato JSON.
- Una vez desanidada la data, se extrajo la informaci√≥n requerida como los nombres de los directores, actores, lenguajes hablados, etc. Los cuales fueron a√±adidos al Dataframe para facilitar consultas posteriores.
- Se eliminaron las columnas que no se iban a utilizar, como ```video```, ```imdb_id```, ```adult```,```original_title```, ```poster_path``` y ```homepage```.
- Se creo una nueva columna llamada ```return``` para permitir consultas relacionadas al retorno de inversi√≥n por pel√≠cula y director.
- Se utiliz√≥ la API de TMDB para impitar datos faltantes en columnas que pod√≠an ser claves para el sistema de recomendaci√≥n de pel√≠culas como ```genre```.
- Se emplearon t√©cnicas como WordNetLemmatizer y word_tokenize para limpiar los caracteres especiales en columnas como ```overview```, evitando la p√©rdida de posibles palabras importantes para el sistema de recomendaci√≥n.
- Finalmente, se realizaron las siguientes importaciones:
  - Toda la data limpia a un archivo .csv llamado üëâ [movies_clean]()
  - Data limpia con s√≥lo las columnas necesarias para las consultas üëâ [api_consultations.csv]()

## üñ•Ô∏è```FastAPI```

Se propone el desarrollo de una API para disponibilizar los datos de la empresa a trav√©s del framework FastAPI. Presentando 6 endpoints, en el archivo üëâ [main.py]() que se consumir√°n en la API. 

Primero se construy√≥ la API de forma local y se configuraron las funciones necesarias para realizar las consultas, cargando la data desde el archivo üëâ [api_consultations.csv]() que se disponibiliz√≥ a trav√©s del ETL.

Estos endpoints son los siguientes:

- ```def cantidad_filmaciones_mes(Mes)```: Consulta que devuelve la cantidad de pel√≠culas que fueron estrenadas en el mes consultado en la totalidad del dataset

        Formato de salida: {'mes': mes, 'cantidad': cantidad}

- ```def cantidad_filmaciones_dia(Dia)```: Consulta que devuelve la cantidad de pel√≠culas que fueron estrenadas en d√≠a consultado en la totalidad del dataset

        Formato de salida: {'dia': dia, 'cantidad': cantidad}

- ```def score_titulo(titulo_de_la_filmaci√≥n)```: Consulta que devuelve el t√≠tulo, el a√±o de estreno y el score de una filmaci√≥n

        Formato de salida: {'titulo': titulo, 'anio': year, 'popularidad': score}

- ```def votos_titulo(titulo_de_la_filmaci√≥n)```: Consulta que devuelve t√≠tulo, la cantidad de votos y el valor promedio de las votaciones de una filmaci√≥n siempre y cuando supere las 2000 valoraciones.

        Formato de salida: {'titulo': titulo, 'a√±o': year, 'voto_total': voto_total, 'voto_promedio': voto_promedio}

- ```def get_actor(nombre_actor)```: Consulta que devuelve el √©xito de un actor medido a trav√©s del retorno de inversi√≥n, as√≠ como la cantidad de pel√≠culas que en las que ha participado y el promedio de retorno.

        Formato de salida: {'actor': nombre_actor, 'cantidad_filmaciones': cantidad_peliculas, 'retorno_total': retorno_total, 'retorno_promedio': retorno_promedio}

- ```def get_director(nombre_director)```: Consulta que devuelve el √©xito de un director medido a trav√©s del retorno de inversi√≥n, nombre de cada pel√≠cula que ha dirigido con la fecha de lanzamiento, retorno individual, costo y ganancia de la misma.

        Formato de salida: {'director': nombre_director,
            'retorno_total_director': retorno_total_director,
            'peliculas': [{
            'titulo': row['title'],
            'anio': row['release_year'],
            'retorno_pelicula': round(row['return'], 4),
            'budget_pelicula': row['budget'],
            'revenue_pelicula': row['revenue']
        }],
            }

Estos endpoints permitir√°n que los empleados de la empresa puedan hacer solicitudes espec√≠ficas a la API para obtener informaci√≥n valiosa o realizar acciones espec√≠ficas.

## üìä ```EDA```

Una vez que los datos fueron limpiados, se realiz√≥ una an√°lisis exploratorio para identificar patrones, relaciones y tendencias en los datos, as√≠ como valores at√≠picos. En este contexto se llevaron a cabo algunas exploraciones interesantes en las siguientes columnas:

- La nube de palabras de las columnas genre, title y overview proporcion√≥ informaci√≥n √∫til permitiendo identificar los g√©neros m√°s populares as√≠ como las palabras m√°s comunes en los t√≠tulos en las descripciones de las pel√≠culas.
- Se utiliz√≥ un histograma de distribuci√≥n de la longitud tanto para los t√≠tulos como para las descripciones, concluyendo que la mayor√≠a de los t√≠tulos son relativamente cortos mientras que para el caso de la columna overview se identific√≥ dos grupos diferentes de res√∫menes con diferentes longitudes.
- Mientras que las columnas como popularity, revenue presentaron una distribuci√≥n fuertemente sesgada debido a valores at√≠picos. Como por ejemplo pel√≠culas que han generado una gran cantidad de ingresos en taquilla y que por otro lado est√°n las que no lograron recaudar o el valor de su ingreso a√∫n no ha sido registrado.
- Hay variables que presentaron una fuerte relaci√≥n positiva como son el caso de revenue y vote_count con 0.81, as√≠ como revenue y budget.
- Por otro lado, se observaron datos interesantes como que John Ford es el director con m√°s n√∫mero de apariciones, los res√∫menes m√°s cortos presentaron una mejor calificaci√≥n promedio, el viernes es el d√≠a predilecto para el lanzamiento de las pel√≠culas, etc. 

## ü§ñ```Machine Learning```

Para implementar el sistema de recomendaci√≥n, se utiliz√≥ la librer√≠a scikit-learn de Python, y se aplic√≥ la t√©cnica de vectorizaci√≥n TF-IDF para crear una matriz de vectores que describ√≠a el contenido de las pel√≠culas en funci√≥n de sus sinopsis. Luego, se utiliz√≥ la medida de similitud del coseno para calcular la similitud entre cada par de pel√≠culas, y se ordenaron las pel√≠culas seg√∫n su score de similaridad.

El resultado final fue una funci√≥n de recomendaci√≥n de pel√≠culas escrita en Python, que toma como entrada el t√≠tulo de una pel√≠cula y devuelve una lista de las 5 pel√≠culas m√°s similares, ordenadas seg√∫n su score de similaridad. La funci√≥n tambi√©n maneja casos en los que el t√≠tulo de la pel√≠cula no se encuentra en la base de datos o cuando hay t√≠tulos de pel√≠culas duplicados que fueron lanzados en a√±os distintos.

Finalmente fue deployado como una funci√≥n adicional de la API, llamada:

- ```def recomendacion(titulo)```: Se ingresa el nombre de una pel√≠cula y recomienda las similares en una lista de 5 valores.

        Formato de salida: ['titulo_recomendado1', 'titulo_recomendado2', 'titulo_recomendado3', 'titulo_recomendado4', 'titulo_recomendado5']

## ```Render```

Para hacer el despliegue de las funciones de la API que incluyen las consultas as√≠ como el sistema de recomendaci√≥n de pel√≠culas se utiliz√≥ Render. El cual permitir√° al equipo de la start-up poder realizar las consultas a trav√©s de una p√°gina web üëâ [render]()

## üìã```Video y recomendaciones```



## üë©‚Äçüíª ```Desarrollador```

![linkedin](https://img.shields.io/badge/linkedin-%230077B5.svg?style=for-the-badge&logo=linkedin&logoColor=white)